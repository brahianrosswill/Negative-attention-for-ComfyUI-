# Negative Attention for ComfyUI

## Introduction

This project provides custom ComfyUI nodes that implement a "negative attention" mechanism. Its purpose is to offer a more nuanced way to guide image generation by calculating and utilizing the difference between positive and negative conditioning in the model's attention layers.

This technique is currently designed and tested for **SDXL and SD 1.X models**. It will not work with Flux (see note at the bottom).

## Core Concept: "Negative Attention"

Traditional image generation workflows use negative prompts to steer the model away from undesired concepts. This project introduces a different approach: "negative attention."

Instead of simply providing a negative prompt to avoid, this method actively:
1.  Computes the attention generated by both the positive and negative prompts.
2.  Calculates the *difference* between these two attention maps.
3.  Uses this difference to modulate the model's attention weights.

This allows for a more refined and direct influence of the negative conditioning, potentially leading to finer control over the final image. The core idea is to let the unconditional prediction be truly unconditional or to emphasize the contrast between positive and negative desires.

This allows the model to:
- Get a negative influence without generating a negative prediction for the unconditional guidance.
- Let the unconditional prediction be truly unconditional.
- Or, optionally, doubling down by having the same "difference" logic applied to the negative prediction input of the sampler (overblown results are to be expected unless using an anti-burn technique or a low CFG scale).

## How Negative Attention Interprets Negative Prompts

The "Negative Attention" mechanism offers a more sophisticated way to utilize negative prompts compared to traditional methods. Instead of simply steering the model *away* from concepts in the negative prompt, it actively uses the negative prompt to refine and emphasize what is desired in the positive prompt.

At its core, the technique focuses on the **difference** or **contrast** between what the positive prompt asks for and what the negative prompt describes. This difference is then used to guide the model.

There are two primary ways this is applied in practice, corresponding to the examples provided:

1.  **Direct Contrast (Primary Method):**
    *   **Setup:** Both positive and negative prompt conditionings are combined by `ConcatSneakyConditioning` (e.g., `[positive_cond, negative_cond]`) and fed to the patched model via its positive conditioning input. The `NegativeAttentionPatchNode` then calculates `attention(positive_cond) - attention(negative_cond)` (scaled by `strength`).
    *   **Effect on Negative Prompt:** The negative prompt directly influences how the positive prompt is interpreted. The model tries to generate an image that embodies the positive prompt *minus* the characteristics of the negative prompt. This can lead to more precise control, as the negative prompt carves out undesired features from the positive concept. For example, if the positive is "apple" and negative is "red", it steers towards non-red apples.
    *   **Which follows better?** This method very directly "follows" the negative prompt by making its content subtractive from the positive prompt's content during the attention calculation. It's about defining the positive by what it *is not* (as specified by the negative).

2.  **Positive Refinement with Separate Sampler-Level Negative:**
    *   **Setup:** The `ConcatSneakyConditioning` node is configured to combine the positive conditioning with an *empty* conditioning (e.g., `[positive_cond, empty_cond]`) for the patched model. A separate, standard negative prompt is then fed to the *sampler's* negative input (e.g., via `negative_out="crop_to_77_tokens"` on `ConcatSneakyConditioning`).
    *   **Effect on Negative Prompt:**
        *   The patched model focuses on `attention(positive_cond) - attention(empty_cond)`. This essentially sharpens or clarifies the positive prompt against a neutral baseline.
        *   The sampler then uses the standard negative prompt in the usual way for unconditional guidance (steering away from broad concepts in the negative prompt globally).
    *   **Which follows better?** This method offers a two-pronged approach. The negative attention part refines the positive prompt in isolation, while the sampler's negative prompt handles broader exclusions. The "following" of the negative prompt is split: the specific negative prompt provided to the sampler is followed in the traditional sense (global avoidance), while the negative attention mechanism itself isn't using that specific negative prompt but rather an empty one to bolster the positive. This can be useful if you want to strongly define the positive concept first, then separately exclude general unwanted elements.

**In summary:**

The **Direct Contrast** method offers a more nuanced and integrated way of "following" the negative prompt by using its content to directly modulate and refine the positive prompt's attention characteristics. It's arguably a more powerful way to use the negative prompt for detailed control.

The **Positive Refinement** method is less about the specific negative prompt directly interacting with the positive in the attention calculation, and more about sharpening the positive while relying on traditional sampler-level negative prompting for general exclusions.

Choosing between them depends on the desired outcome:
*   For fine-grained control where the negative prompt defines specific aspects to remove or contrast *within* the positive concept, the **Direct Contrast** method is generally more effective at "following" the negative prompt's detailed intent.
*   If the goal is to have a strong positive focus while generally avoiding common undesirable elements, the **Positive Refinement** setup combined with a standard sampler negative prompt can be very effective, though the "negative attention" part itself isn't using the main negative prompt.

## Overview of Custom Nodes

This project introduces two main custom nodes:

1.  **`ConcatSneakyConditioning` (Node Name: "Negative cross attention concatenate")**
    *   **Purpose:** This node preprocesses and combines the positive and negative conditioning tensors from your prompts.
    *   **Outputs:**
        *   `Positive (Combined)`: A special concatenated tensor containing both positive and negative conditioning information, intended for the *model's positive conditioning input*.
        *   `Negative (Sampler)`: The original negative conditioning, intended for the *sampler's negative conditioning input*.

2.  **`NegativeAttentionPatchNode` (Node Name: "Negative cross attention")**
    *   **Purpose:** This node patches the model's attention mechanism. It modifies the relevant attention layers to correctly interpret the combined conditioning produced by `ConcatSneakyConditioning`.
    *   **Mechanism:** Inside the patched attention layers, the combined conditioning is split. The difference in attention between the positive and negative parts is then calculated and applied, achieving the "negative attention" effect.

## How it Works (High-Level Workflow)

The typical data flow for using these nodes is as follows:

1.  **Input Prompts:** Provide your positive and negative prompts as usual.
2.  **`ConcatSneakyConditioning` Node:**
    *   Connect your positive prompt's conditioning output to the `positive` input of this node.
    *   Connect your negative prompt's conditioning output to the `negative` input of this node.
3.  **Node Outputs:**
    *   The `Positive (Combined)` output from `ConcatSneakyConditioning` is connected to the **model's positive conditioning input** (which is then used by the `NegativeAttentionPatchNode` internally).
    *   The `Negative (Sampler)` output from `ConcatSneakyConditioning` is connected to the **sampler's negative conditioning input** (e.g., the `negative` input on a KSampler node).
4.  **`NegativeAttentionPatchNode` Node:**
    *   This node is applied directly to the model.
    *   It should be placed *after* the model loader and *before* the sampler in your workflow.
    *   The output of this node (the patched model) is then connected to the **sampler's model input** (e.g., the `model` input on a KSampler node).

![Example node setup for Negative Attention](https://github.com/user-attachments/assets/c43caf96-8f43-4c1c-8813-9a70a646f3cd)
*Conceptual diagram showing the `ConcatSneakyConditioning` node providing conditioning to the KSampler's positive and negative inputs, and the `NegativeAttentionPatchNode` patching the model which is then fed to the KSampler's model input.*

The patched model, when used by the KSampler, will internally use the special combined conditioning (received via its positive conditioning input) to calculate attention differences and guide the generation process. The KSampler also uses the direct negative conditioning from `ConcatSneakyConditioning` for its standard unconditional guidance.

Like any model patcher, the `NegativeAttentionPatchNode` is to be plugged right after the model loader:

![Patcher Placement](https://github.com/user-attachments/assets/a27d9796-e563-4661-985e-4ee53c37ebb0)

An example workflow (`example_workflow.json`) is provided in the repository.

## Installation

1.  Navigate to your ComfyUI `custom_nodes` directory:
    ```bash
    cd ComfyUI/custom_nodes/
    ```
2.  Clone this repository:
    ```bash
    git clone https://github.com/Extraltodeus/Negative-attention-for-ComfyUI- Negative-attention-for-ComfyUI
    ```
3.  Restart ComfyUI.

The nodes "Negative cross attention concatenate" and "Negative cross attention" should now be available in your ComfyUI node menu.

## Examples

The following examples illustrate the effect of using this negative attention mechanism. These examples typically use the node setup described in the "How it Works (High-Level Workflow)" section.

**No modification (standard ComfyUI behavior):**
A baseline image generated without any negative attention modifications.
![Standard ComfyUI generation - baseline](https://github.com/user-attachments/assets/537999fd-a594-4eb9-ad60-28c4958172ea)

**Using Negative Attention (difference between positive and negative conditionings):**
This shows the primary effect. The `ConcatSneakyConditioning` node combines both positive and negative prompt embeddings into its `Positive` output (fed to the patched model). The `NegativeAttentionPatchNode` (with a given `strength`) then calculates the difference. The `Negative` output of `ConcatSneakyConditioning` (e.g., set to `crop_to_77_tokens` or `empty_or_0` if the original negative prompt is also desired for the sampler's unconditional guidance) is fed to the KSampler's negative input. The model actively steers the generation by emphasizing what is unique to the positive prompt compared to the negative prompt.
![Image generated using Negative Attention difference method](https://github.com/user-attachments/assets/471f2b3f-53be-41aa-a940-5ee3eacb57d5)

**Using Negative Attention (model sees positive-vs-empty, sampler sees original negative):**
This example demonstrates a specific setup:
*   The `positive` input to `ConcatSneakyConditioning` is a regular positive prompt.
*   The `negative` input to `ConcatSneakyConditioning` is an *empty* conditioning (e.g., from an empty text prompt).
*   The `concat_mode` is set to accommodate these (e.g., `prolongate_to_longest_with_empty_or_0`).
*   The `Positive` output of `ConcatSneakyConditioning` (now `[positive_conditioning, empty_conditioning]`) goes to the patched model's positive input. The `NegativeAttentionPatchNode` will calculate the difference between the positive prompt and an empty concept.
*   The `negative_out` on `ConcatSneakyConditioning` is set to `crop_to_77_tokens` (or similar, if you have a *separate, actual* negative prompt you want the sampler to use for its standard unconditional guidance). If you want the sampler's unconditional guidance to also be based on an empty prompt, you could use `empty_or_0` and provide an empty conditioning to the `empty` input of the node.
The image shows the result of the model focusing on the difference between the positive prompt and nothingness, while the sampler separately uses a standard negative prompt.
![Image generated with positive-vs-empty for model, and standard negative for sampler](https://github.com/user-attachments/assets/af14ad61-8640-42b8-82ef-143dded04f10)

**No modification using empty negative conditioning (standard ComfyUI behavior):**
For comparison, this is how standard ComfyUI behaves with an empty negative prompt, without any negative attention nodes.
![Standard ComfyUI generation with empty negative prompt](https://github.com/user-attachments/assets/729bdeed-2dfe-4c87-923a-aa0eb5294e45)

## Note on Flux Compatibility

I haven't managed to make this work with anything but SDXL / SD1.5 based models.

I did spend two hours looking for how to patch the equivalent of the cross attention for Flux but did not find how (like the keywords for the patch or something).

Any help appreciated! If you have experience with Flux's internal architecture and know how to adapt this patching technique, please feel free to contribute or reach out.

## Node: `Negative cross attention` (`NegativeAttentionPatchNode`)

### Purpose
Patches the model's attention mechanism to incorporate the negative attention logic. This node modifies the model to interpret specially prepared conditioning tensors, calculate attention based on the difference between positive and negative prompts, and apply this nuanced attention during the generation process.

### Inputs
*   **`model`**:
    *   **Type:** `MODEL`
    *   **Description:** The input model to be patched. This is typically the model loaded by a checkpoint loader.
*   **`strength`**:
    *   **Type:** `FLOAT`
    *   **Description:** A multiplier that controls the intensity of the calculated negative attention difference. Higher values give more weight to the negative attention effect.
    *   **Default:** `1.0`
    *   **Min:** `0.0`
    *   **Max:** `100.0`
    *   **Step:** `0.25`
    *   **Round:** `0.001`

### Outputs
*   **`Model`**:
    *   **Type:** `MODEL`
    *   **Description:** The patched model with the modified attention mechanism. This output model should be used in subsequent parts of the KSampler or other sampling processes.

### How it Works Internally
This node performs several key operations:

1.  **Clones Model:** It first creates a clone of the input model to avoid modifying the original model instance.
2.  **NaN/Inf Interrupt Patch:** It applies a `nan_interrupt_patch` to the cloned model. This is a safety feature that monitors the denoised output during sampling. If NaN (Not a Number) or Inf (Infinity) values are detected, it interrupts the current processing to prevent errors or corrupted outputs.
3.  **Attention Mechanism Replacement:**
    *   The core function of this node is to replace the standard attention mechanism (specifically `attn2` which handles cross-attention) in specified layers of the model. It targets layers in the "input", "middle", and "output" blocks of the U-Net.
    *   It uses the `set_model_attn2_replace` method to substitute the original attention function with a custom function: `attention_patch.attention_with_negative`.
4.  **Custom Attention Logic (`attention_patch.attention_with_negative`):**
    *   This custom function is where the "negative attention" is calculated. When the model executes an attention operation in a patched layer, this function is called.
    *   **Conditioning Splitting:** It assumes that the key (`k`) and value (`v`) tensors passed to it are special concatenated tensors (usually prepared by the `ConcatSneakyConditioning` node). These tensors contain both the positive and negative conditioning information, stacked along the sequence length dimension. The function splits these `k` and `v` tensors in half to separate the positive parts (first half) from the negative parts (second half). If the conditioning sequence length indicates it's not a doubled tensor, it falls back to standard `optimized_attention`.
    *   **Scaled Dot-Product Attention with Negative (`scaled_dot_product_attention_with_negative`):** This is the heart of the negative attention calculation.
        *   It computes the standard attention weights and resulting values for the *positive* part of the conditioning: `positive_output = softmax(query @ positive_key.T * scale_factor + bias) @ positive_value`. (Note: `scale_factor` and `bias` are part of the typical attention calculation but simplified in the final step for clarity in the README).
        *   It computes the standard attention weights and resulting values for the *negative* part of the conditioning: `negative_output = softmax(query @ negative_key.T * scale_factor + bias) @ negative_value`.
        *   It then calculates the *difference* between these two outputs: `diff_val = positive_output - negative_output`.
        *   This `diff_val` represents the unique influence of the negative prompt relative to the positive one.
        *   The `diff_val` is projected by a linear layer (`proj`) to match the dimension of attention scores (raw logits before softmax).
        *   This projected difference (`projected_diff = proj(diff_val)`), scaled by the `strength` parameter, is then added to the original attention logits calculated for the positive conditioning: `final_attn_logits = (query @ positive_key.T * scale_factor + bias) + projected_diff * strength`.
        *   The final attention weights are computed by applying softmax: `final_attn_weight = softmax(final_attn_logits)`.
        *   The final output of the attention mechanism is then `final_attn_weight @ positive_value`.

## Node: `Negative cross attention concatenate` (`ConcatSneakyConditioning`)

### Purpose
Prepares and concatenates positive and negative conditioning tensors. This node is designed to work in conjunction with the `NegativeAttentionPatchNode`. It creates a special combined conditioning tensor that the patched model can split and use for the negative attention calculation, and also provides a separate negative conditioning output for the sampler.

### Inputs
*   **`positive`**:
    *   **Type:** `CONDITIONING`
    *   **Description:** The positive conditioning, typically from a CLIP text encoder.
*   **`negative`**:
    *   **Type:** `CONDITIONING`
    *   **Description:** The negative conditioning, typically from a CLIP text encoder.
*   **`concat_mode`**:
    *   **Type:** `STRING` (Dropdown menu)
    *   **Description:** Defines how to handle differences in length (number of tokens) between the `positive` and `negative` conditioning tensors before concatenation.
    *   **Default:** `crop_to_shortest`
    *   **Options:**
        *   `crop_to_shortest`: Truncates the longer conditioning tensor to match the length of the shorter one.
        *   `prolongate_to_longest_by_loop`: Repeats the tokens of the shorter conditioning tensor until it matches the length of the longer one.
        *   `prolongate_to_longest_with_empty_or_0`: Pads the shorter conditioning tensor. If the `empty` input is provided, this `empty` conditioning (often a standard 77-token tensor from an empty prompt) is used as a source for padding tokens (potentially tiled by repeating the `empty` conditioning if it's shorter than the required padding length, though typically it's long enough or repeated to cover any needed length). Otherwise, zero vectors are used for padding.
*   **`negative_out`**:
    *   **Type:** `STRING` (Dropdown menu)
    *   **Description:** Defines the content of the second `Negative` output conditioning tensor.
    *   **Default:** `empty_or_0`
    *   **Options:**
        *   `empty_or_0`: Outputs a tensor of zeros with the standard 77 token length (if `empty` input is not provided) or the `empty` conditioning input (also cropped/padded to 77 tokens). This is useful if you want the sampler's unconditional guidance to be neutral.
        *   `invert`: Outputs a tensor where the concatenated positive and negative conditionings (from the first output) are swapped. (The original positive becomes negative, and the original negative becomes positive).
        *   `crop_to_77_tokens`: Outputs the original `negative` conditioning input, cropped or padded to the standard 77 token length.
*   **`empty`** (optional):
    *   **Type:** `CONDITIONING`
    *   **Description:** An optional conditioning tensor (e.g., from an empty prompt) used for padding by `prolongate_to_longest_with_empty_or_0` `concat_mode`, and potentially by the `empty_or_0` `negative_out` mode.

### Outputs
*   **`Positive`**:
    *   **Type:** `CONDITIONING`
    *   **Description:** The main output tensor where the `positive` and `negative` input conditionings have been processed (according to `concat_mode`) and then concatenated together (e.g., `[positive_processed, negative_processed]`). This combined tensor is specifically designed to be fed into the model that has been patched by `NegativeAttentionPatchNode`. The patched attention mechanism will split this tensor internally.
*   **`Negative`**:
    *   **Type:** `CONDITIONING`
    *   **Description:** The secondary output tensor whose content is determined by the `negative_out` parameter. This output is typically connected to the negative conditioning input of a KSampler node.

### How it Works Internally
1.  **Conditioning Preparation (`combine_conds` method):**
    *   The node first processes the input `positive` and `negative` conditioning tensors based on the selected `concat_mode`:
        *   **`crop_to_shortest`:** Determines the minimum length between the positive and negative tensors and truncates the longer one to this length.
        *   **`prolongate_to_longest_by_loop`:** Determines the maximum length. If one tensor is shorter, its content is repeated (looped) until it reaches this maximum length.
        *   **`prolongate_to_longest_with_empty_or_0`:** Determines the maximum length. If one tensor is shorter, it's padded. If the `empty` input is provided, its content (potentially tiled by repetition if the `empty` conditioning is shorter than the padding needed) is used for padding. Otherwise, zero vectors are used.
    *   After adjusting lengths, the processed positive and negative tensors are concatenated along the token dimension (sequence length) to form the `Positive` output tensor.

2.  **Negative Output Generation (`negative_out` logic):**
    *   The content of the `Negative` output tensor is determined by the `negative_out` setting:
        *   **`invert`:** The `Positive` output tensor (which is `[processed_pos, processed_neg]`) is cloned, and its two halves are swapped, resulting in `[processed_neg, processed_pos]`.
        *   **`empty_or_0`:** If the `empty` input conditioning is provided, its first 77 tokens are used. Otherwise, a tensor of zeros (shape typically `[1, 77, embedding_dim]`) is created.
        *   **`crop_to_77_tokens`:** The original `negative` input conditioning is taken and cropped or padded to ensure it has 77 tokens.

## Code Structure and Internals

This section provides a brief overview of key files and components within the project, offering insight into how the custom nodes are structured and integrated with ComfyUI.

### `__init__.py`

*   **Purpose:** This standard Python file signifies that the directory containing it should be treated as a Python package.
*   **Role in Project:** Its primary function in this ComfyUI custom node project is to define the `NODE_CLASS_MAPPINGS` dictionary. This dictionary is crucial for ComfyUI to discover and register the custom nodes. It maps user-friendly display names for the nodes (e.g., "Negative cross attention") to their corresponding Python class implementations (e.g., `NegativeAttentionPatchNode`, `ConcatSneakyConditioning` located in `nodes.py`).

    ```python
    # Example from __init__.py:
    NODE_CLASS_MAPPINGS = {
        "Negative cross attention": NegativeAttentionPatchNode,
        "Negative cross attention concatenate": ConcatSneakyConditioning,
    }
    ```

### `pyproject.toml`

*   **Purpose:** This file is a standard configuration file for Python projects, adhering to PEP 518. It's used to define project metadata and build system requirements.
*   **Content:**
    *   **Project Information:** It includes basic information such as the project's `name` (`negative-attention-for-comfyui-`), `version`, `description`, `license` details, and `[project.urls]` like the `Repository` URL.
    *   **ComfyUI Specific Configuration:** The `[tool.comfy]` section contains metadata specifically for the ComfyUI ecosystem. This can be used by tools like the ComfyUI Manager or the Comfy Registry to display information such as `PublisherId`, `DisplayName`, and an `Icon` (though the icon is currently not set).

    ```toml
    # Example snippet from pyproject.toml:
    [project]
    name = "negative-attention-for-comfyui-"
    version = "1.0.0"

    [tool.comfy]
    PublisherId = "extraltodeus"
    DisplayName = "Negative-attention-for-ComfyUI-"
    ```

### `attention_patch` class (in `nodes.py`)

*   **Purpose:** This is a helper class defined within the `nodes.py` file. It encapsulates the core custom attention logic that modifies how the model processes attention when the negative attention mechanism is active.
*   **Functionality:**
    *   An instance of `attention_patch` is created within the `patch` method of the `NegativeAttentionPatchNode`, initialized with the `negative_strength` value provided to the node.
    *   Its primary method, `attention_with_negative`, is designed to replace the model's original cross-attention function (`attn2`).
    *   When this method is called (during the model's forward pass, after patching), it first checks if the conditioning tensor (`k`) has been doubled (i.e., contains both positive and negative conditioning). If not, it defaults to ComfyUI's `optimized_attention`.
    *   If it is a doubled tensor, `attention_with_negative` splits the key (`k`) and value (`v`) tensors into their positive and negative halves.
    *   It then calls the `scaled_dot_product_attention_with_negative` function (also in `nodes.py`), passing these separated components along with the `negative_strength`, to compute the final attention output based on the difference logic described in the `NegativeAttentionPatchNode` section.
